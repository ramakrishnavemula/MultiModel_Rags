# -*- coding: utf-8 -*-
"""Multimodel_Rag_CLIP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p5t1ghM4fokW85dTA08uUmuhVViNuP2a
"""

pip install pymupdf langchain_core langchain_community

# --- PDF and image processing ---
import fitz  # PyMuPDF
from PIL import Image
import io
import os
import base64

# --- Core libraries ---
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# --- LangChain ecosystem ---
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chat_models import init_chat_model

# --- Multimodal embedding / vision models ---
from transformers import CLIPProcessor, CLIPModel

#Initalizing clip(Contrastive Language-Image Pre-Training) model for unified embeddings

#clip model used to bridge the gap between computer vision and natural language processing by creating a shared embedding space for images and text
#clip_processor is used for preparing input data (images and text) for the CLIP model by handling necessary preprocessing steps like image resizing, normalization, and text tokenization.

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

clip_model.eval()

def embed_image(image_data):
  if isinstance(image_data, str):
    image = Image.open(image_data).convert("RGB")
  else:
    image = image_data

  inputs = clip_processor(images = image, return_tensors = "pt")

  with torch.no_grad():
    featuers = clip_model.get_image_features(**inputs)

    featuers = featuers / featuers.norm(dim = -1, keepdim = True)
    return featuers.squeeze().numpy()

def embed_text(text):
  inputs = clip_processor(
      text = text,
      return_tensors = "pt",
      padding  = True,
      truncation = True,
      max_length = 77
)

  with torch.no_grad():
    featuers = clip_model.get_text_features(**inputs)

    featuers = featuers/ featuers.norm(dim = -1, keepdim = True)
    return featuers.squeeze().numpy()

path = "/content/multimodal_sample (1).pdf"
doc = fitz.open(path)

all_docs = []
all_embeds = []
all_images = {}

splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)

if doc.is_closed:
    doc = fitz.open(path)
for i, page in enumerate(doc):
  text = page.get_text()
  text.strip()

  temp_doc = Document(page_content = text, metadata = {"page" : i, "type": "text"})
  text_chnks = splitter.split_documents([temp_doc])

  for chunk in text_chnks:
    embedding = embed_text(chunk.page_content)
    all_embeds.append(embedding)
    all_docs.append(chunk)

for img_index, img in enumerate(page.get_images(full=True)):
        try:
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]

            # Convert to PIL Image
            pil_image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

            # Create unique identifier
            image_id = f"page_{i}_img_{img_index}"

            # Store image as base64 for later use with GPT-4V
            buffered = io.BytesIO()
            pil_image.save(buffered, format="PNG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            all_images[image_id] = img_base64

            # Embed image using CLIP
            embedding = embed_image(pil_image)
            all_embeds.append(embedding)

            # Create document for image
            image_doc = Document(
                page_content=f"[Image: {image_id}]",
                metadata={"page": i, "type": "image", "image_id": image_id}
            )
            all_docs.append(image_doc)

        except Exception as e:
            print(f"Error processing image {img_index} on page {i}: {e}")
            continue

all_docs

embeddings_array = np.array(all_embeds)
embeddings_array

(all_docs,embeddings_array)

vector_store = FAISS.from_embeddings(
    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],
    embedding=None,  # Already computed embeddings
    metadatas=[doc.metadata for doc in all_docs]
)
vector_store

import os
os.environ["OPENAI_API_KEY"] ="your api key"

llm = init_chat_model("openai:gpt-4.1")
llm

def retrieve_multimodal(query, k=5):
    # Embed query using CLIP
    query_embedding = embed_text(query)

    # Search in unified vector store
    results = vector_store.similarity_search_by_vector(
        embedding=query_embedding,
        k=k
    )

    return results

def create_multimodal_message(query, retrieved_docs, all_images):

    content = []

    # Add the query
    content.append({
        "type": "text",
        "text": f"Question: {query}\n\nContext:\n"
    })

    # Separate text and image documents
    text_docs = [doc for doc in retrieved_docs if doc.metadata.get("type") == "text"]
    image_docs = [doc for doc in retrieved_docs if doc.metadata.get("type") == "image"]

    # Add text context
    if text_docs:
        text_context = "\n\n".join([
            f"[Page {doc.metadata['page']}]: {doc.page_content}"
            for doc in text_docs
        ])
        content.append({
            "type": "text",
            "text": f"Text excerpts:\n{text_context}\n"
        })

    # Add images
    for doc in image_docs:
        image_id = doc.metadata.get("image_id")
        if image_id and image_id in all_images:
            content.append({
                "type": "text",
                "text": f"\n[Image from page {doc.metadata['page']}]:\n"
            })
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{all_images[image_id]}"
                }
            })

    # Add instruction
    content.append({
        "type": "text",
        "text": "\n\nPlease answer the question based on the provided text and images."
    })

    return HumanMessage(content=content)

def multimodal_pdf_rag_pipeline(query):
    # Retrieve relevant documents
    context_docs = retrieve_multimodal(query, k=5)

    # Create multimodal message
    message = create_multimodal_message(query, context_docs,all_images)

    # Get response from GPT-4V
    response = llm.invoke([message])

    # Print retrieved context info
    print(f"\nRetrieved {len(context_docs)} documents:")
    for doc in context_docs:
        doc_type = doc.metadata.get("type", "unknown")
        page = doc.metadata.get("page", "?")
        if doc_type == "text":
            preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
            print(f"  - Text from page {page}: {preview}")
        else:
            print(f"  - Image from page {page}")
    print("\n")

    return response.content

if __name__ == "__main__":
    # Example queries
    queries = [
        "What does the chart on page 1 show about revenue trends?",
        "Summarize the main findings from the document",
        "What visual elements are present in the document?"
    ]

    for query in queries:
        print(f"\nQuery: {query}")
        print("-" * 50)
        answer = multimodal_pdf_rag_pipeline(query)
        print(f"Answer: {answer}")
        print("=" * 70)